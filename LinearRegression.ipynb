{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> \n",
       "table {margin-left: 0 !important;}\n",
       "th, tr, td { text-align: center; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<style> \n",
    "table {margin-left: 0 !important;}\n",
    "th, tr, td { text-align: center; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear Regression is an algorithm used in **Supervised** Machine Learning to use input's ($x$'s) given from a labelled dataset to create predictions ($\\hat{y}$'s) which result in the lowest difference possible when compared to the actual ouput ($y$). The algorithm used to find the innacuracy (loss) of a linear regression model is known as a **cost function**.\n",
    "\n",
    "## Model Representation\n",
    "\n",
    "![Model Representation](./assets/images/ModelRepresentation.png)\n",
    "<center>*Model representation which will be used as reference to explain how unilinear regression will work*</center>\n",
    "\n",
    "### Linear regression works in a way where:\n",
    "1. You are given a **Training Set** which you will use in order to form a learning algorithm.\n",
    "\n",
    "2. Our **Learning Algorithm** creates an algorithm (hypothesis) which consists of the input and parameters ($\\theta_0$, $\\theta_1$) which need to be weighted in order to get the lowest possible loss when the predicted y ($\\hat{y}$) ouputs are compared to the correct ones ($y$) in the training set.\n",
    "\n",
    "3. The inputs ($x$) are then passed through the hypothesis and the result of the loss are plotted on a contour graph after some defined number of epochs will show us which weight values for $\\theta_0$, $\\theta_1$ led to the smallest loss.\n",
    "\n",
    "### For Unilinear Regression we will need the following algorithms:\n",
    "\n",
    "#### Learning Algorithm (Hypothesis)\n",
    "$h_\\theta (x) = \\theta_0 + \\theta_1 • x$ \n",
    "\n",
    "This is the hypothesis. A linear equation ($y = mx + b$) where the y-intercept is $\\theta_0$ and the slope is $\\theta_1$. These two parameters $\\theta_0, \\theta_1$ will be the weights we modify to get the most accurate line of best fit for our data that incur the least amount of loss.\n",
    "\n",
    "| Properties    | Value          | Description                                                       |\n",
    "|---------------|----------------|-------------------------------------------------------------------|\n",
    "| Input         | $x$            | This is the inputted value we will try map to a predicted y value.|\n",
    "| Y-intercept   | $\\theta_0$     | The point on the y-axis the line of best fit will go through.     |\n",
    "| Slope         | $\\theta_1$     | The regression coefficient (the slope of the line of best fit).   |\n",
    "| Output        | $y$            | The predicted y value for the x input.                            |\n",
    "\n",
    "#### Cost Function\n",
    "$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta (x^i) - y^i)^2$\n",
    "\n",
    "This is the cost function which will is used to get the **mean-squared error** of our predictions. It does this by summing up the squared difference of each prediction and correct value ($y - \\hat{y}$). Once we get the sum of all our squared differences we divide them by $\\frac{1}{2 • m}$ where $m$ is the number of rows we have in our data set. Once this is all done we should have the mean error of our hypothesis.\n",
    "\n",
    "| Properties    | Value                   | Description                                                       |\n",
    "|---------------|-------------------------|-------------------------------------------------------------------|\n",
    "| Input         | $h_\\theta (x)$          | This is our prediction ($\\hat{y}$) which we wil compred to $y$.   |\n",
    "| y             | $y$                     | The point on the y-axis the line of best fit will go through.     |\n",
    "| Output        | $J(\\theta_0, \\theta_1)$ | The mean loss as a real number.                                   |\n",
    "\n",
    "## Worked Example (Written)\n",
    "\n",
    "\n",
    "<img src=\"./assets/images/LinearRegressionGraph1.png\"> \n",
    "\n",
    "Here we have a training set which has plotted onto a graph. Using what we talked about above we are now going to use Unilinear regression to figure out what the best $\\theta_0$ (y-intercept) and $\\theta_1$ (slope) fits the data with the least loss.\n",
    "\n",
    "To do this we need to use the learning algorithm and cost function which have been shown above. First off let's try set the parameter $\\theta_0$ to 0 and $\\theta_1$ to 1 and see what sort of results we get.\n",
    "\n",
    "$h_\\theta(x) = 0 + 1 • x$\n",
    "\n",
    "The results this yields can be see on the graph below where $\\theta_0$ is set to 0 and $\\theta_1$ to 1.\n",
    "\n",
    "\n",
    "<img src=\"./assets/images/LinearRegressionGraph2.png\"> \n",
    "\n",
    "Here you can see the graph showing how our line of best fit would look like when the wights of our parameters ($\\theta_0$, $\\theta_0$) are set to the following: $\\theta_0$ = 0, $\\theta_1$ = 1.\n",
    "\n",
    "The line of best fit itself is defined by the hypothesis $h_\\theta(x)$ and the lines between the plotted $x$'s connecting down to the line of best fit is our cost function $J(\\theta_0, \\theta_1)$ which signifies the loss (inaccuracy) of our hypothesis.\n",
    "\n",
    "$h_\\theta(x) = 0 + 1 • x$\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = \\frac{1}{2 • 5}(0.5^2 + 0.5^2 + 0.5^2 + 0.5^2 + 0.5^2) = \\frac{1}{10}(2.25) = 0.225$\n",
    "\n",
    "From the calculations above we know that our hypothesis with the current weights given gives us a mean squared error of 0.225 meaning our predictions are innacurate form the actual values ($x$) by an average of **0.225**.\n",
    "\n",
    "<img src=\"./assets/images/LinearRegressionGraph3.png\"> \n",
    "\n",
    "Above is the graph which incurs no loss and in this case is a perfect line of best fit for our data. When the parameters $\\theta_0$ is set to 0.5 and $\\theta_1$ = 1 we get a hypothesis which leads to the cost function returning an average loss of 0 meaning our predictions are perfect for this data and leads to perfect predictions for future data for this dataset.\n",
    "\n",
    "This is a very simple example of linear regression with one variable and in most cases there would never be a perfect line of best fit for a dataset.\n",
    "\n",
    "### How to Find The Best Parameters Based on Cost Function Results\n",
    "\n",
    "Let's say we want the learning algorithm to run for 100 epochs where we change the parameters of our hypothesis simultaneously by 0.1 each time we then get the cost function result back which gives us the mean-squared error and we plot that on a contour plot which will allow us to visualise after 100 epochs which parameters led to the smallest mean-squared error. Once found we can decide to use these specific values of $\\theta_0$ and $\\theta_1$ on our dataset to make predictions.\n",
    "\n",
    "This process is known as **Gradient Descent** which I will cover in more details in another guide in this revision book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
