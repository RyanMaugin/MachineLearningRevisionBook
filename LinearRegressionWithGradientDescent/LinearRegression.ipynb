{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> table {margin-left: 0 !important;} th, tr, td { text-align: center; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "<style> table {margin-left: 0 !important;} th, tr, td { text-align: center; } </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression With Gradient Descent\n",
    "**<p style=\"color: red;\">I recognise that this may not be the best technique for implmenting linear regression but I am using it for the simplicity of learning gradient descent. However this implementation is useful for scaling to many different features.</p>**\n",
    "\n",
    "Linear Regression is an algorithm used in **Supervised** Machine Learning to use input's ($x$'s) given from a labelled dataset to create predictions ($\\hat{y}$'s) which result in the lowest difference possible when compared to the actual ouput ($y$). The algorithm used to find the innacuracy (loss) of a linear regression model is known as a **cost function**.\n",
    "\n",
    "## Model Representation\n",
    "\n",
    "![Model Representation](./../assets/images/ModelRepresentation.png)\n",
    "<center>*Model representation which will be used as reference to explain how unilinear regression will work*</center>\n",
    "\n",
    "### Linear regression works in a way where:\n",
    "1. You are given a **Training Set** which you will be use to form a learning algorithm with based on the features in the dataset.\n",
    "\n",
    "2. Our **Learning Algorithm** creates an algorithm (hypothesis) which consists of the input and parameters ($\\theta_0$, $\\theta_1$) which need to be weighted in order to get the lowest possible loss when the predicted y ($\\hat{y}$) ouputs are compared to the correct ones ($y$) in the training set.\n",
    "\n",
    "3. The inputs ($x$) are then passed through the hypothesis and the result of the loss are plotted on a contour graph after some defined number of epochs will show us which weight values for $\\theta_0$, $\\theta_1$ that led to the smallest loss.\n",
    "\n",
    "### For Unilinear Regression we will need the following algorithms:\n",
    "\n",
    "#### Learning Algorithm (Hypothesis)\n",
    "$h_\\theta (x) = \\theta_0 + \\theta_1 \\cdot x$ \n",
    "\n",
    "This is the hypothesis. A linear equation ($y = mx + b$) where the y-intercept is $\\theta_0$ and the slope is $\\theta_1$. These two parameters $\\theta_0, \\theta_1$ will be weighted to get the most accurate line of best fit for our data that incur the least amount of loss.\n",
    "\n",
    "| Properties    | Value          | Description                                                       |\n",
    "|---------------|----------------|-------------------------------------------------------------------|\n",
    "| Input         | $x$            | This is the inputted value we will try map to a predicted y value.|\n",
    "| Y-intercept   | $\\theta_0$     | The point on the y-axis the line of best fit will go through.     |\n",
    "| Slope         | $\\theta_1$     | The regression coefficient (the slope of the line of best fit).   |\n",
    "| Output        | $y$            | The predicted y value for the x input.                            |\n",
    "\n",
    "#### Cost Function\n",
    "$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta (x^i) - y^i)^2$\n",
    "\n",
    "This is the cost function which will is used to get the **mean-squared error** of our predictions. It does this by summing up the squared difference of each prediction and correct value ($y - \\hat{y}$). Once we get the sum of all our squared differences we multiply them by $\\frac{1}{2 \\cdot m}$ where $m$ is the number of rows we have in our data set. Once this is all done we should have the mean error of our hypothesis.\n",
    "\n",
    "| Properties    | Value                   | Description                                                       |\n",
    "|---------------|-------------------------|-------------------------------------------------------------------|\n",
    "| Input         | $h_\\theta (x)$          | This is our prediction ($\\hat{y}$) which we wil compred to $y$.   |\n",
    "| y             | $y$                     | The point on the y-axis the line of best fit will go through.     |\n",
    "| Output        | $J(\\theta_0, \\theta_1)$ | The mean loss as a real number.                                   |\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "repeat until convergence {\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$\n",
    "\n",
    "$\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1)$\n",
    "\n",
    "}\n",
    "\n",
    "The Gradient Descent algorithm is used to find the minimum of a function by taking small proportionate changes to $\\theta_0, \\theta_1$ defined by a learning rate ($\\alpha$) in order to reach a local or global minimum value of a function which in this case is $J(\\theta_0, \\theta_1)$. This algorithm will stop when it cannot go any lower no matter what direction it tries to go which in this case the minimum is found.\n",
    "\n",
    "## Written Example of Linear Regression\n",
    "\n",
    "\n",
    "<img src=\"./../assets/images/LinearRegressionGraph1.png\"> \n",
    "\n",
    "Here we have a training set which has plotted onto a graph. Using what we talked about above we are now going to use Unilinear regression to figure out what the best $\\theta_0$ (y-intercept) and $\\theta_1$ (slope) fits the data with the least loss.\n",
    "\n",
    "To do this we need to use the learning algorithm and cost function which have been shown above. First off let's try set the parameter $\\theta_0$ to 0 and $\\theta_1$ to 1 and see what sort of results we get.\n",
    "\n",
    "$h_\\theta(x) = 0 + 1 \\cdot x$\n",
    "\n",
    "The results this yields can be see on the graph below where $\\theta_0$ is set to 0 and $\\theta_1$ to 1.\n",
    "\n",
    "\n",
    "<img src=\"./../assets/images/LinearRegressionGraph2.png\"> \n",
    "\n",
    "Here you can see the graph showing how our line of best fit would look like when the wights of our parameters ($\\theta_0$, $\\theta_0$) are set to the following: $\\theta_0$ = 0, $\\theta_1$ = 1.\n",
    "\n",
    "The line of best fit itself is defined by the hypothesis $h_\\theta(x)$ and the lines between the plotted $x$'s connecting down to the line of best fit is our cost function $J(\\theta_0, \\theta_1)$ which signifies the loss (inaccuracy) of our hypothesis.\n",
    "\n",
    "$h_\\theta(x) = 0 + 1 \\cdot x$\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = \\frac{1}{2 \\cdot 5}(0.5^2 + 0.5^2 + 0.5^2 + 0.5^2 + 0.5^2) = \\frac{1}{10}(2.25) = 0.225$\n",
    "\n",
    "From the calculations above we know that our hypothesis with the current weights given gives us a mean squared error of 0.225 meaning our predictions are innacurate form the actual values ($x$) by an average of **0.225**.\n",
    "\n",
    "<img src=\"./../assets/images/LinearRegressionGraph3.png\"> \n",
    "\n",
    "Above is the graph which incurs no loss and in this case is a perfect line of best fit for our data. When the parameters $\\theta_0$ is set to 0.5 and $\\theta_1$ = 1 we get a hypothesis which leads to the cost function returning an average loss of 0 meaning our predictions are perfect for this data and leads to perfect predictions for future data for this dataset.\n",
    "\n",
    "The working for this would look something like this:\n",
    "\n",
    "$h_\\theta(x) = 0.5 + 1 \\cdot x$\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = \\frac{1}{2 \\cdot 5}(0^2 + 0^2 + 0^2 + 0^2 + 0^2) = \\frac{1}{10}(0) = 0$\n",
    "\n",
    "<img width=\"200\" height=\"200\" src=\"./../assets/images/ContourPlot.png\">\n",
    "\n",
    "The image above shows a 3D contour plot which is what we would use to plot the results of our cost function. The contour plot would have the parameters $\\theta_0, \\theta_1$ along the $x$ and $y$ axis and the result of $J(\\theta_0, \\theta_1)$ will be on the *z* axis to map the loss for an $x$ and $y$ pair. We then use gradient descent in order to find the local or global minimum of this $J(\\theta_0, \\theta_1)$ function.\n",
    "\n",
    "<img width=\"300\" height=\"400\" src=\"http://blog.datumbox.com/wp-content/uploads/2013/10/gradient-descent.png\">\n",
    "\n",
    "Here we see how gradient descent works on a more complex dataset where it is initialised at 2 different points and from these points when gradient descent is run two different local minimums are reached. The way gradient descent will work on our dataset is by taking $J(\\theta_0, \\theta_1)$ and getting the partial derivative of the slope to determine the direction we will move in to reduce the loss we get. The result of this is then multiplied by the learning rate in order to define the size of the changes we make to our $\\theta_0, \\theta_1$ parameters in order to not overshoot our minimum by setting the learning rate too high or take too long to converge by setting it too low.\n",
    "\n",
    "<center style=\"color: red;\"><b>I will explain and implement this in more details in the coded example</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
